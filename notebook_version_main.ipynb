{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from hyperopt import hp, fmin, tpe\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "random.seed(499)\n",
    "import shap\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "##################################### functions which are used for preprocessing #######################################\n",
    "\n",
    "\n",
    "# function which imputes missing values by mean or median values\n",
    "def impute(df, by, method='mean'):\n",
    "    if method == 'mean':\n",
    "        return df.fillna(by.mean())\n",
    "    elif method == 'median':\n",
    "        return df.fillna(by.median())\n",
    "    # if method not valid then raise error\n",
    "    else:\n",
    "        raise ValueError(\"Imputation method not allowed!\\n - Please choose from ['mean','median']\")\n",
    "\n",
    "\n",
    "# load datasets into python\n",
    "def load_files(dir_name):\n",
    "    dfs = []\n",
    "    for file in os.listdir(dir_name):\n",
    "        dfs.append(pd.read_csv(dir_name + file))\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# convert long format into wide format\n",
    "def long2wide(dfs, col, value, index=None):\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i] = dfs[i].pivot(index=index, columns=col, values=value)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# function which extracts features from datasets and concat all datasets together\n",
    "def feature_extract(dfs, static_col):\n",
    "    mean = pd.DataFrame()\n",
    "    maximum = pd.DataFrame()\n",
    "    minimum = pd.DataFrame()\n",
    "    # extract maximum, minimum and mean values\n",
    "    for df in dfs:\n",
    "        mean = mean.append(df.mean(), ignore_index=True)\n",
    "        maximum = maximum.append(df.max(), ignore_index=True)\n",
    "        minimum = minimum.append(df.min(), ignore_index=True)\n",
    "    # leave static data out\n",
    "    static = mean[static_col]\n",
    "    mean = mean.drop(static_col, axis=1).add_suffix('_mean')\n",
    "    maximum = maximum.drop(static_col, axis=1).add_suffix('_max')\n",
    "    minimum = minimum.drop(static_col, axis=1).add_suffix('_min')\n",
    "    # concat three dataframes forming the entire data used for train, test or validation\n",
    "    return pd.concat([static, mean, maximum, minimum], axis=1, sort=False)\n",
    "\n",
    "# remove all features with 0 variance\n",
    "def non0var(df):\n",
    "    # first need to separate categorical data and numerical data\n",
    "    categorical = df.select_dtypes(include='object')\n",
    "    numerical = df.select_dtypes(exclude='object')\n",
    "    # only numerical data have variance\n",
    "    numerical = numerical.iloc[:, list(numerical.var() != 0)]\n",
    "    df = pd.concat([categorical, numerical], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# function which makes three dataframes the same\n",
    "def synchronize(train, validation, test):\n",
    "    # delete columns which are not common\n",
    "    test = test.drop(list(set(test).difference(set(train))), axis=1)\n",
    "    test = test.drop(list(set(test).difference(set(validation))), axis=1)\n",
    "    validation = validation.drop(list(set(validation).difference(set(train))), axis=1)\n",
    "    validation = validation.drop(list(set(validation).difference(set(test))), axis=1)\n",
    "    train = train.drop(list(set(train).difference(set(test))), axis=1)\n",
    "    train = train.drop(list(set(train).difference(set(validation))), axis=1)\n",
    "    return train, validation, test\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "###################################### functions which are used for modelling ##########################################\n",
    "\n",
    "\n",
    "# function which is used to tune hyperparameters, can be a customized function\n",
    "def objective(params):\n",
    "    bst = xgb.XGBClassifier(max_depth=int(params['max_depth']), learning_rate=params['learning_rate'],\n",
    "                            n_estimators=int(params['n_estimators']), gamma=params['gamma'],\n",
    "                            min_child_weight=params['min_child_weight'], max_delta_step=params['max_delta_step'],\n",
    "                            subsample=params['subsample'],\n",
    "                            reg_alpha=params['reg_alpha'], reg_lambda=params['reg_lambda'],\n",
    "                            scale_pos_weight=params['scale_pos_weight'])\n",
    "    bst.fit(train_X, train_y)\n",
    "    yhat = bst.predict(validation_X)\n",
    "    # use the same evaluation metric with the competition\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(validation_y, yhat).ravel()\n",
    "    # optimization can only find minimum hence use 1 - score\n",
    "    return 1 - min(tp / (tp + fn), tp / (tp + fp))\n",
    "\n",
    "\n",
    "# evaluate the model by testing data\n",
    "def model_evaluation(model):\n",
    "    model.fit(train_X, train_y)\n",
    "    yhat = model.predict(test_X)\n",
    "    # use the same metrics with the comprtition\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(test_y, yhat).ravel()\n",
    "    return min(tp / (tp + fn), tp / (tp + fp)), model\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "######################################### high-level functions for the project #########################################\n",
    "\n",
    "\n",
    "# the default preprocessing strategy\n",
    "def default_preprocess(dir_name):\n",
    "    # load data\n",
    "    dfs = load_files(dir_name)\n",
    "    # change format from long to wide\n",
    "    dfs = long2wide(dfs, 'Parameter', 'Value')\n",
    "    # extract features\n",
    "    df = feature_extract(dfs, ['RecordID', 'Gender', 'Age', 'Height', 'ICUType'])\n",
    "    # replace -1 by missing\n",
    "    df = df.replace(-1, np.NaN)\n",
    "    return df\n",
    "\n",
    "\n",
    "# the default strategy for modelling\n",
    "def default_modelling(space, objective, max_evals):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # find the best hyperparameters\n",
    "    param = fmin(objective, space, algo=tpe.suggest, max_evals=max_evals, rstate=np.random.RandomState(499))\n",
    "    # write to a text file for the use of application\n",
    "    df = open('param.txt', 'w')\n",
    "    df.write(str(param))\n",
    "    df.close()\n",
    "    # construct the model according to the hyperparameters chosen\n",
    "    model = xgb.XGBClassifier(max_depth=int(param['max_depth']), learning_rate=param['learning_rate'],\n",
    "                              n_estimators=int(param['n_estimators']), gamma=param['gamma'],\n",
    "                              min_child_weight=param['min_child_weight'], max_delta_step=param['max_delta_step'],\n",
    "                              subsample=param['subsample'], reg_alpha=param['reg_alpha'],\n",
    "                              reg_lambda=param['reg_lambda'],\n",
    "                              scale_pos_weight=param['scale_pos_weight'])\n",
    "    # evaluate the model and print the score1\n",
    "    score1, model = model_evaluation(model)\n",
    "    return score1, model\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "##################################### functions for explainer ##########################################################\n",
    "\n",
    "\n",
    "# construct the explainer using the model and testing data\n",
    "def construct_explainer(model, test_X):\n",
    "    shap.initjs()\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(test_X)\n",
    "    return explainer, shap_values\n",
    "\n",
    "\n",
    "# plot the summary for top features\n",
    "def summary(shap_values, test_X):\n",
    "    shap.summary_plot(shap_values, test_X)\n",
    "\n",
    "\n",
    "# plot the effect for specified feature\n",
    "def effect(feature_name, shap_values, test_X):\n",
    "    shap.dependence_plot(feature_name, shap_values, test_X, interaction_index=None)\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "########################################################################################################################\n",
    "######################################## main function of the project ##################################################\n",
    "\n",
    "\n",
    "# load datasets\n",
    "train_X = default_preprocess('set-a/')\n",
    "train_X = impute(non0var(train_X), train_X)\n",
    "print('Training set ready to use!')\n",
    "validation_X = default_preprocess('set-b/')\n",
    "validation_X = impute(non0var(validation_X), validation_X)\n",
    "print('Validation set ready to use!')\n",
    "test_X = default_preprocess('set-c/')\n",
    "test_X = impute(non0var(test_X), train_X)\n",
    "print('test set ready to use!')\n",
    "train_X, validation_X, test_X = synchronize(train_X, validation_X, test_X)\n",
    "print('Datasets sychronized!')\n",
    "\n",
    "# load labels\n",
    "train_y = pd.read_csv('Outcomes-a.txt')[['RecordID', 'In-hospital_death']]\n",
    "validation_y = pd.read_csv('Outcomes-b.txt')[['RecordID', 'In-hospital_death']]\n",
    "test_y = pd.read_csv('Outcomes-c.txt')[['RecordID', 'In-hospital_death']]\n",
    "\n",
    "# merge datasets\n",
    "train = pd.merge(train_X, train_y, on='RecordID')\n",
    "validation = pd.merge(validation_X, validation_y, on='RecordID')\n",
    "test = pd.merge(test_X, test_y, on='RecordID')\n",
    "\n",
    "# generate X and y\n",
    "test_X = test.drop(['In-hospital_death', 'RecordID'], axis=1)\n",
    "test_X = test_X.reindex(sorted(test_X), axis=1)\n",
    "train_X = train.drop(['In-hospital_death', 'RecordID'], axis=1)\n",
    "train_X = train_X.reindex(sorted(train_X), axis=1)\n",
    "validation_X = validation.drop(['In-hospital_death', 'RecordID'], axis=1)\n",
    "validation_X = validation_X.reindex(sorted(validation_X), axis=1)\n",
    "test_y = test['In-hospital_death']\n",
    "train_y = train['In-hospital_death']\n",
    "validation_y = validation['In-hospital_death']\n",
    "\n",
    "# define the search space\n",
    "space = {'max_depth': hp.quniform('max_depth', 2, 5, 1),\n",
    "         'learning_rate': hp.uniform('learning_rate', 0.01, 0.1),\n",
    "         'n_estimators': hp.quniform('n_estimators', 80, 150, 1),\n",
    "         'gamma': hp.uniform('gamma', 0, 10),\n",
    "         'min_child_weight': hp.uniform('min_child_weight', 0, 5),\n",
    "         'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n",
    "         'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "         'reg_alpha': hp.uniform('reg_alpha', 0, 10),\n",
    "         'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "         'scale_pos_weight': hp.uniform('scale_pos_weight', 3, 5)\n",
    "         }\n",
    "print('Model starts tuning!')\n",
    "\n",
    "# tune the model such that it reach its best performance\n",
    "score1, model = default_modelling(space, objective, 200)\n",
    "# show score1\n",
    "print(score1)\n",
    "\n",
    "# construct explainer\n",
    "explainer, shap_values = construct_explainer(model, test_X)\n",
    "# plot the summary for top features\n",
    "summary(shap_values, test_X)\n",
    "# plot the effect of age\n",
    "effect('Age', shap_values, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the individual explanation\n",
    "shap.force_plot(explainer.expected_value, shap_values[1000,:], test_X.iloc[1000,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
